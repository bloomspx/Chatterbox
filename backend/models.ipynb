{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup, UnicodeDammit\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import pdfplumber\n",
    "import re\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import base64\n",
    "import io, json\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNA sample article urls\n",
    "# url = \"https://www.channelnewsasia.com/sustainability/scientists-discover-how-air-pollution-triggers-lung-cancer-2931111\"\n",
    "# url = \"https://www.channelnewsasia.com/asia/indonesia-subsidised-fuel-price-increase-micro-businesses-2929006\"\n",
    "url = \"https://www.channelnewsasia.com/singapore/nightclub-operator-club-posh-west-palace-entertainment-evade-gst-money-laundering-jailed-penalty-2935436\"\n",
    "# url = \"https://cnaluxury.channelnewsasia.com/people/chye-seng-huat-hardware-leon-foo-morning-coffee-machine-206531\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "\n",
    "def generate_sentiments(messageJson):\n",
    "\n",
    "    # sentiment analysis model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    tokenizer.save_pretrained(\"./models/roberta\")\n",
    "    model.save_pretrained(\"./models/roberta\")\n",
    "\n",
    "### TEST CODE\n",
    "requestJson = {\"text\": 'text'}\n",
    "sentimentJson = generate_sentiments(requestJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert CNA url -> HTML -> CSV\n",
    "def extract_csv_from_html(url):\n",
    "    try:\n",
    "        page=requests.get(url) \n",
    "    except Exception as e:    \n",
    "        error_type, error_obj, error_info = sys.exc_info()      \n",
    "        print ('ERROR FOR LINK:',url)                     \n",
    "        print (error_type, 'Line:', error_info.tb_lineno)\n",
    "\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "\n",
    "    ## extract article content\n",
    "    textContent = soup.find_all('div', attrs={'class':'text'})\n",
    "    paragraphs = []\n",
    "\n",
    "    for i in textContent:\n",
    "        para = i.find_all('p')\n",
    "        for j in para:\n",
    "            content = j.getText().strip()\n",
    "            paragraphs.append(content)\n",
    "\n",
    "    df = pd.DataFrame(paragraphs)\n",
    "    df.to_csv(\"backend/data/news.csv\", index=False)\n",
    "\n",
    "\n",
    "message = extract_csv_from_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert cna url -> html -> txt\n",
    "def extract_txt_from_html(url):\n",
    "    try:\n",
    "        page=requests.get(url) \n",
    "    except Exception as e:    \n",
    "        error_type, error_obj, error_info = sys.exc_info()      \n",
    "        print ('ERROR FOR LINK:',url)                     \n",
    "        print (error_type, 'Line:', error_info.tb_lineno)\n",
    "\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "    ## extract article title\n",
    "    title = soup.find('h1',attrs={'class':'h1 h1--page-title'}).getText()\n",
    "\n",
    "    ## extract article content\n",
    "    textContent = soup.find_all('div', attrs={'class':'text'})\n",
    "    paragraphs = []\n",
    "\n",
    "    for i in textContent:\n",
    "        para = i.find_all('p')\n",
    "        for j in para:\n",
    "            content = j.getText().strip()\n",
    "            paragraphs.append(content)\n",
    "\n",
    "    single_para = \"\".join(paragraphs)\n",
    "    # lines = \"\\n\".join(paragraphs)\n",
    "    # multi_para = \"\\n\".join(paragraphs)\n",
    "\n",
    "    f = open(\"backend/data/article.txt\", \"w+\")\n",
    "    f.write(single_para)\n",
    "    f.close()\n",
    "\n",
    "message = extract_txt_from_html(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts content from csv/txt in /data to an entire paragraph\n",
    "def extract_text(filename):\n",
    "    message = \"\"\n",
    "    name, extension = os.path.splitext(filename)\n",
    "    try: \n",
    "        if extension == \".csv\":   # csv format\n",
    "            with open(\"data/{}\".format(filename), 'r', encoding=\"utf8\") as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader, None)  # skip headers\n",
    "                for row in reader:\n",
    "                    # print(row)    \n",
    "                    message += \"\".join(row)\n",
    "                # print(sentences)\n",
    "        elif extension == \".txt\":     # txt format\n",
    "            with open(\"data/{}\".format(filename), 'rb') as f:\n",
    "                byteString = f.read()\n",
    "                dammit = UnicodeDammit(byteString, [\"utf-8\",  \"ascii\"])\n",
    "                encoding = dammit.original_encoding\n",
    "                print(\"Encoding Type:\", encoding)\n",
    "                message = byteString.decode(encoding, errors='ignore')\n",
    "            \n",
    "        elif extension == '.pdf':\n",
    "            with pdfplumber.open(\"data/{}\".format(filename)) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    message += page.extract_text()        \n",
    "    except Exception as err:\n",
    "            print(err, \"occured in\"+filename)\n",
    "\n",
    "    dir_path = os.path.abspath('')\n",
    "    nltk.data.path.append(dir_path + '/models/nltk_data')\n",
    "    print(\"sentences: \", len(sent_tokenize(message))) \n",
    "    print(\"words: \", len(word_tokenize(message))) \n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LATEST NLP METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Type: utf-8\n",
      "Researchers from the National University of Singapore observed the data of 4,275 women with a history of gestational diabetes mellitus. Of these, 924 women had developed type 2 diabetes over a follow-up period of 28 years. A normal body mass index of 18.5 to 24.9, a high-quality diet, regular exercise, abstinence from smoking and moderate alcohol consumption were considered modifiable risk factors.\n"
     ]
    }
   ],
   "source": [
    "## GENERATE SUMMARY: if text exceeds 512 tokens, it is cut off\n",
    "def generate_summary(messageJson):\n",
    "    # Summarization model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"models/bart-summary\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"models/bart-summary\")\n",
    "\n",
    "    text = messageJson[\"text\"]\n",
    "\n",
    "    # encode text into tensor of integers\n",
    "    inputs = tokenizer.encode(\"summarize:\" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(\n",
    "                    inputs, \n",
    "                    max_length=150, \n",
    "                    min_length=40, \n",
    "                    length_penalty=2.0, \n",
    "                    num_beams=4, \n",
    "                    early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(summary)\n",
    "    \n",
    "    returnJson = {\"summary\": summary}\n",
    "    return returnJson\n",
    "\n",
    "\n",
    "test_filename = \"article3.txt\"\n",
    "text = extract_text(test_filename)\n",
    "requestJson = {\"text\": text}\n",
    "summarizedJson = generate_summary(requestJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Sentence and Word Split\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def generate_sentiments(messageJson):\n",
    "    dir_path = os.path.abspath('')\n",
    "    nltk.data.path.append(dir_path + '/models/nltk_data')\n",
    "\n",
    "    # sentiment analysis model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"models/roberta-SA\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"models/roberta-SA\")\n",
    "    model.save(\"./roberta\")\n",
    "    labels=['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "    message = messageJson[\"text\"]\n",
    "    sentences = message.split('.')\n",
    "    nltk_sentences = sent_tokenize(message)\n",
    "    nltk_words = word_tokenize(message)\n",
    "    print(\"NLTK sentence count:\", len(nltk_sentences))\n",
    "    print(message.split(' '))\n",
    "    print(\"NLTK word count:\", len(nltk_words))\n",
    "    print(nltk_words)\n",
    "\n",
    "### TEST CODE\n",
    "test_filename = \"article2.txt\"\n",
    "text = extract_text(test_filename)\n",
    "num_words = len(text.split(' '))\n",
    "num_sent = len(text.split('.'))\n",
    "print(\"Total wordcount:\", num_words)\n",
    "print(\"Total sentences:\", num_sent)\n",
    "requestJson = {\"text\": text}\n",
    "sentimentJson = generate_sentiments(requestJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Type: utf-8\n",
      "sentences:  8\n",
      "words:  407\n",
      "[('diabetes', 0.5276), ('obesity', 0.4081), ('diet', 0.3756), ('pregnancy', 0.291), ('health', 0.2517), ('prevalence', 0.236), ('risk', 0.1615), ('gdm', 0.1435)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'Topics',\n",
       " 'color': 'hsl(13, 70%, 50%)',\n",
       " 'children': [{'id': 'diabetes', 'value': 0.528},\n",
       "  {'id': 'obesity', 'value': 0.408},\n",
       "  {'id': 'diet', 'value': 0.376},\n",
       "  {'id': 'pregnancy', 'value': 0.291},\n",
       "  {'id': 'health', 'value': 0.252},\n",
       "  {'id': 'prevalence', 'value': 0.236},\n",
       "  {'id': 'risk', 'value': 0.162},\n",
       "  {'id': 'gdm', 'value': 0.143}]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KEYBERT ALTERNATIVE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk, random\n",
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "\n",
    "def generate_topics(document, no_topics=3, no_top_words=10):\n",
    "\n",
    "    modelPath = dir_path + '/models/all-MiniLM-L6-v2' \n",
    "    sentence_model = SentenceTransformer(modelPath)\n",
    "    kw_model = KeyBERT(model=sentence_model)\n",
    "\n",
    "    topics_words = kw_model.extract_keywords(document,\n",
    "                                    # keyphrase_ngram_range=(1, 2),\n",
    "                                    use_mmr=True,\n",
    "                                    diversity=0.3,\n",
    "                                    stop_words=\"english\",\n",
    "                                    # vectorizer=KeyphraseCountVectorizer(),\n",
    "                                    vectorizer=KeyphraseCountVectorizer(pos_pattern='<N.*>'),\n",
    "                                    top_n=8)\n",
    "    print(topics_words)\n",
    "\n",
    "    # Altering data structure to pass to frontend \n",
    "    outJson = {\n",
    "        'name': 'Topics',\n",
    "        'color': \"hsl(13, 70%, 50%)\",\n",
    "        'children': []\n",
    "    }\n",
    "\n",
    "    t = 0\n",
    "    for word in topics_words:\n",
    "        t = t + 1\n",
    "        word_vis = {\n",
    "            \"id\": word[0],\n",
    "            \"value\": round(word[1],3)\n",
    "        }\n",
    "        outJson['children'].append(word_vis)\n",
    "    with open(dir_path +  '/output/topics.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(outJson, f, ensure_ascii=False, indent=4) \n",
    "    return outJson\n",
    "\n",
    "### TEST CODE    \n",
    "dir_path = os.path.abspath('')\n",
    "nltk.data.path.append(dir_path + '/models/nltk_data')\n",
    "\n",
    "# f = open(dir_path + \"/output/results.json\")\n",
    "# data = json.load(f)\n",
    "# corpus = [doc[\"text\"] for doc in data]\n",
    "\n",
    "test_filename = \"article3.txt\"\n",
    "document = extract_text(test_filename)\n",
    "# data = [json[\"text\"]]\n",
    "# data = sent_tokenize(document)\n",
    "generate_topics(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences:  47\n",
      "words:  1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.46it/s]\n",
      "2022-10-07 09:54:07,842 - BERTopic - Transformed documents to Embeddings\n",
      "2022-10-07 09:54:09,446 - BERTopic - Reduced dimensionality\n",
      "2022-10-07 09:54:09,451 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BerTopic:\n",
      "Keybert:\n",
      "[{'id': 'hoarding', 'value': 0.597}, {'id': 'hoarders', 'value': 0.581}, {'id': 'hoarder', 'value': 0.52}, {'id': 'decluttering', 'value': 0.507}, {'id': 'containers', 'value': 0.303}, {'id': 'debris', 'value': 0.293}, {'id': 'permission', 'value': 0.182}, {'id': 'singapore', 'value': 0.181}]\n"
     ]
    }
   ],
   "source": [
    "# RUN BERTOPIC FIRST FOLLOWED BY KEYBERT\n",
    "# BERTOPIC RUNS RANDOMLY UPON MULTIPLE TRIALS, UNLESS U SET UMAP \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk, random\n",
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "def generate_topics(document, no_topics=3, no_top_words=10):\n",
    "\n",
    "    failBert = False\n",
    "    global topics_words\n",
    "\n",
    "    modelPath = dir_path + '/models/all-MiniLM-L6-v2' \n",
    "    sentence_model = SentenceTransformer(modelPath)\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "    # OPTIMALLY RUN BERTOPIC\n",
    "    if (failBert == False):\n",
    "        try:\n",
    "            data = sent_tokenize(document)\n",
    "            topic_model = BERTopic(calculate_probabilities=True,\n",
    "                                diversity=0.2,\n",
    "                                embedding_model=sentence_model,\n",
    "                                vectorizer_model=vectorizer_model,\n",
    "                                verbose=True)\n",
    "\n",
    "            topics, probabilities = topic_model.fit_transform(data)\n",
    "            topics_words = topic_model.get_topic(0)\n",
    "            print(\"BerTopic:\")\n",
    "            if (topics_words == False):\n",
    "                failBert = True\n",
    "        except Exception as e:\n",
    "            failBert = True\n",
    "            print(e) \n",
    "        \n",
    "    if (failBert):\n",
    "        # KEYBERT ALTERNATIVE FOR SHORTER DOCUMENTS\n",
    "        kw_model = KeyBERT(model=sentence_model)\n",
    "\n",
    "        topics_words = kw_model.extract_keywords(document,\n",
    "                                        # keyphrase_ngram_range=(1, 2),\n",
    "                                        use_mmr=True,\n",
    "                                        diversity=0.3,\n",
    "                                        stop_words=\"english\",\n",
    "                                        # vectorizer=KeyphraseCountVectorizer(),\n",
    "                                        vectorizer=KeyphraseCountVectorizer(pos_pattern='<N.*>'),\n",
    "                                        top_n=8)\n",
    "        print(\"Keybert:\")\n",
    "\n",
    "    # Altering data structure to pass to frontend \n",
    "    outJson = []\n",
    "\n",
    "    for word in topics_words:\n",
    "        word_vis = {\n",
    "            \"id\": word[0],\n",
    "            \"value\": round(word[1],3)\n",
    "        }\n",
    "        outJson.append(word_vis)\n",
    "\n",
    "    with open(dir_path +  '/output/topics.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(outJson, f, ensure_ascii=False, indent=4) \n",
    "    return outJson\n",
    "\n",
    "\n",
    "\n",
    "### TEST CODE    \n",
    "dir_path = os.path.abspath('')\n",
    "nltk.data.path.append(dir_path + '/models/nltk_data')\n",
    "\n",
    "test_filename = \"article_long.pdf\"\n",
    "document = extract_text(test_filename)\n",
    "topics = generate_topics(document)\n",
    "print(topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Type: utf-8\n",
      "sentences:  66\n",
      "words:  1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  2.93it/s]\n",
      "2022-10-06 11:18:20,241 - BERTopic - Transformed documents to Embeddings\n",
      "2022-10-06 11:18:22,671 - BERTopic - Reduced dimensionality\n",
      "2022-10-06 11:18:22,681 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hpb', 0.12494788212355505), ('salt', 0.09470908114110338), ('healthier', 0.08204789041552257), ('sodium', 0.0748586857336087), ('taste', 0.04864694923546847), ('alternatives', 0.04864694923546847), ('ingredient', 0.04496652293126789), ('chefs', 0.04496652293126789), ('lee', 0.0398366411605192), ('sauces', 0.03801727986976176)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.04864694923546847,
          0.0748586857336087,
          0.08204789041552257,
          0.09470908114110338,
          0.12494788212355505
         ],
         "xaxis": "x",
         "y": [
          "taste  ",
          "sodium  ",
          "healthier  ",
          "salt  ",
          "hpb  "
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#0072B2"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.0665015157340014,
          0.072087717113043,
          0.09119205955699322,
          0.13714945428760125,
          0.16467159591953137
         ],
         "xaxis": "x2",
         "y": [
          "msg  ",
          "potassium  ",
          "singapore  ",
          "sodium  ",
          "salt  "
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 0",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 1",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 325,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Topic Word Scores",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "showgrid": true
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "showgrid": true
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "showgrid": true
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          1
         ],
         "showgrid": true
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import umap, hdbscan\n",
    "\n",
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "\n",
    "### TEST CODE\n",
    "    \n",
    "dir_path = os.path.abspath('')\n",
    "nltk.data.path.append(dir_path + '/models/nltk_data')\n",
    "\n",
    "test_filename = \"article_long.txt\"\n",
    "document = extract_text(test_filename)\n",
    "# data = [json[\"text\"]]\n",
    "data = sent_tokenize(document)\n",
    "\n",
    "# ---------------------- #\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "modelPath = dir_path + '/models/all-MiniLM-L6-v2' \n",
    "sentence_model = SentenceTransformer(modelPath)\n",
    "# embeddings = sentence_model.encode(data, show_progress_bar=False)\n",
    "\n",
    "# Create BERTopic model\n",
    "topic_model = BERTopic(calculate_probabilities=True,\n",
    "                    diversity=0.2,\n",
    "                    embedding_model=sentence_model,\n",
    "                    vectorizer_model=vectorizer_model,\n",
    "                    verbose=True)\n",
    "\n",
    "topics, probabilities = topic_model.fit_transform(data)\n",
    "topic_info = topic_model.get_topic(0)\n",
    "print(topic_info)\n",
    "topic_model.visualize_barchart()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Topic  Count                              Name\n",
    "0     -1     57  -1_mental_health_services_people\n",
    "1      0     32      0_mental_health_covid19_prof\n",
    "2      1     23       1_mental_health_people_care\n",
    "3      2     20    2_services_care_support_health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return (texts_out)\n",
    "\n",
    "def generate_topics(json, no_topics=3, no_top_words=10):\n",
    "    dir_path = os.path.abspath('')\n",
    "    nltk.data.path.append(dir_path + '/models/nltk_data')\n",
    "    \n",
    "    data = [json[\"text\"]]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    lemmatized_texts = lemmatization(data)\n",
    "    print(lemmatized_texts)\n",
    "\n",
    "\n",
    "### TEST CODE\n",
    "test_filename = \"covid.txt\"\n",
    "text = extract_text(test_filename)\n",
    "requestJson = {\"text\": text}\n",
    "sentimentJson = generate_topics(requestJson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizes entire paragraph and exports as JSON\n",
    "def summarize(summarizer, chunks):\n",
    "    result = \"\"\n",
    "    for i in chunks:\n",
    "        summarized = summarizer(i, max_length=70, min_length=30, do_sample=False)\n",
    "        # print(summarized[0][\"summary_text\"])\n",
    "        result += summarized[0][\"summary_text\"]\n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "def generate_summary(messageJson):\n",
    "    # Summarization model\n",
    "    model = \"facebook/bart-large-cnn\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "    message = messageJson[\"text\"]\n",
    "    sentences = message.split('.')\n",
    "    chunks = form_text_chunks(sentences, 1024)\n",
    "    # print(\"chunks:\", chunks)\n",
    "    summary = summarize(summarizer, chunks)\n",
    "    # print(result + \"\\n\")\n",
    "    while (len(summary) > 1200):\n",
    "        sentences = summary.split('.')\n",
    "        chunks = form_text_chunks(sentences, 1024)\n",
    "        summary = summarize(summarizer, chunks)\n",
    "    print(summary)\n",
    "    returnJson = {\"summary\": summary}\n",
    "    # print(\"ReturnJSON:\", returnJson)\n",
    "    return returnJson\n",
    "\n",
    "\n",
    "### TEST CODE\n",
    "test_filename = \"article.txt\"\n",
    "text = extract_text(test_filename)\n",
    "num_words = len(text.split(' '))\n",
    "print(\"Total wordcount:\", num_words)\n",
    "requestJson = {\"text\": text}\n",
    "summarizedJson = generate_summary(requestJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentiments(message):\n",
    "\n",
    "    nltk.data.path.append(dir_path + '/models/nltk_data')\n",
    "\n",
    "    # sentiment analysis model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(dir_path + \"/models/roberta-SA\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(dir_path + \"/models/roberta-SA\")\n",
    "    \n",
    "    labels=['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "    sentences = sent_tokenize(message)\n",
    "    \n",
    "    scores_table = []\n",
    "    sentimentJson = {\"overall_score\":{}, \"overall_sentiment\": \"none\", \"sentiment_count\":{}}\n",
    "    sentiment_count = {\"Negative\": 0, \"Neutral\": 0, \"Positive\": 0}\n",
    "    sentiment_by_sent = {}\n",
    "    \n",
    "    for index in range(len(sentences)):\n",
    "        text = sentences[index]\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        output = model(**encoded_input)\n",
    "        scores = output[0][0].detach().tolist()\n",
    "        scores = softmax(scores)    # converts into probabilities\n",
    "\n",
    "        best_score = np.round(np.max(scores), 4)\n",
    "        scores_table.append(scores)\n",
    "        pred_sentiment = labels[np.argmax(scores)]\n",
    "        sentiment_by_sent[index] = {\"text\":text, \"score\":best_score, \"sentiment\":pred_sentiment}\n",
    "        sentiment_count[pred_sentiment] += 1\n",
    "    \n",
    "    # calculate overall average sentiment       \n",
    "    np_scores = np.asarray(scores_table)\n",
    "    # print((np_scores))\n",
    "    avg_sentiments = np.round(np.average(np_scores, axis=0), 4)\n",
    "    for index in range(len(avg_sentiments)):\n",
    "        sentiment = labels[index]\n",
    "        sentimentJson[\"overall_score\"][sentiment] = avg_sentiments[index]\n",
    "    sentimentJson[\"overall_sentiment\"] = labels[np.argmax(avg_sentiments)]\n",
    "    sentimentJson[\"sentiment_count\"] = sentiment_count\n",
    "    sentimentJson[\"sentiment_distribution\"] = sentiment_by_sent\n",
    "    print(sentimentJson)\n",
    "    return sentimentJson\n",
    "\n",
    "### TEST CODE\n",
    "test_filename = \"covid.pdf\"\n",
    "text = extract_text(test_filename)\n",
    "sentimentJson = generate_sentiments(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(messageJson):\n",
    "    message = messageJson[\"text\"]\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", width=1600, height=800)\n",
    "    wc.generate(message)\n",
    "    imageRes = wc.to_image()\n",
    "    \n",
    "    # display wordcloud\n",
    "    plt.figure()\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Convert to bytestring \n",
    "    file_object = io.BytesIO()\n",
    "    imageRes.save(file_object, format='PNG')\n",
    "    bytestring = base64.b64encode(file_object.getvalue())\n",
    "    returnJson = {\"data\": bytestring.decode('utf-8')}\n",
    "\n",
    "    with open(\"wordcloud.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(returnJson, f)\n",
    "\n",
    "    print(returnJson)\n",
    "    return returnJson\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "### TEST CODE\n",
    "test_filename = \"sample_news.csv\"\n",
    "text = extract_text(test_filename)\n",
    "num_words = len(text.split(' '))\n",
    "num_sent = len(text.split('.'))\n",
    "print(\"Total wordcount:\", num_words)\n",
    "print(\"Total sentences:\", num_sent)\n",
    "requestJson = {\"text\": text}\n",
    "wordcloudJson = generate_word_cloud(requestJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "with open(\"backend/data/dataurl.txt\", \"r\", encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    paragraph = \"\".join(lines)\n",
    "    index = paragraph.index(',')\n",
    "    url = paragraph[index+1:]\n",
    "    print(base64.b64decode(url))\n",
    "    with open(\"backend/data/test.txt\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text & export NLP results as JSON\n",
    "def run_chatterbox(filename):\n",
    "    text = extract_text(filename)\n",
    "    num_words = len(text.split(' '))\n",
    "    num_sent = len(text.split('.'))\n",
    "    requestJson = {\"text\": text}\n",
    "\n",
    "    print(\"Total wordcount:\", num_words)\n",
    "    print(\"Total sentences:\", num_sent)\n",
    "    \n",
    "    # Sentiment\n",
    "    sentimentJson = generate_sentiments(requestJson)\n",
    "    # Summary\n",
    "    summarizedJson = generate_summary(requestJson)\n",
    "    # Word Cloud\n",
    "    wordcloudJson = generate_word_cloud(requestJson)\n",
    "    \n",
    "    finalJson = {**sentimentJson, **summarizedJson, **wordcloudJson}\n",
    "    \n",
    "    return finalJson\n",
    "\n",
    "response = run_chatterbox(\"article.txt\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export individual text, sentiment and score into JSON\n",
    "def export_sentiments_to_JSON(chunks, model, tokenizer):\n",
    "        scores_table = []\n",
    "        new_df = pd.DataFrame(columns=[\"Content\",\"Sentiment\", \"Score\"])\n",
    "        for index in range(len(chunks)):\n",
    "                text = chunks[index]\n",
    "                encoded_input = tokenizer(text, return_tensors='pt')\n",
    "                output = model(**encoded_input)\n",
    "                scores = output[0][0].detach().tolist()\n",
    "                scores = softmax(scores)    # converts into probabilities\n",
    "                \n",
    "                scores_table.append(scores)\n",
    "                max_index = np.argmax(scores)\n",
    "                pred_sentiment = labels[max_index]\n",
    "                pred_score = scores[pred_sentiment]\n",
    "\n",
    "                # write text and predicted data into df\n",
    "                new_df.at[index, \"Sentiment\"] = pred_sentiment\n",
    "                new_df.at[index, \"Score\"] = pred_score\n",
    "                new_df.at[index, \"Content\"] = \"\".join((chunks[index]))\n",
    "        new_df.to_json(\"backend/data/results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD SENTINENT ANALYSIS METHOD WITH PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentiments(messageJson):\n",
    "    # sentiment analysis model\n",
    "    # model = \"siebert/sentiment-roberta-large-english\"\n",
    "    nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=model)\n",
    "\n",
    "    message = messageJson[\"text\"]\n",
    "    texts = message.split('.')\n",
    "        \n",
    "    print(texts, \"\\n\\n\")\n",
    "    chunks = form_text_chunks(texts, 512)\n",
    "    new_df = pd.DataFrame(columns=[\"Content\",\"Sentiment\", \"Score\"])\n",
    "\n",
    "    for index in range(len(chunks)):\n",
    "        preds = nlp(chunks[index])\n",
    "        # print(preds)\n",
    "        pred_sentiment = preds[0][\"label\"]\n",
    "        pred_score = preds[0][\"score\"]\n",
    "\n",
    "        # write predicted data into df\n",
    "        new_df.at[index, \"Sentiment\"] = pred_sentiment\n",
    "        new_df.at[index, \"Score\"] = pred_score\n",
    "        # write text\n",
    "        new_df.at[index, \"Content\"] = \"\".join((chunks[index]))\n",
    "\n",
    "    # new_df.to_csv(\"backend/data/results.csv\", index=False)\n",
    "    new_df.to_json(\"backend/data/results.json\")\n",
    "    returnJson = {\"sentiment\": new_df}\n",
    "    print(returnJson)\n",
    "    return returnJson\n",
    "    return None\n",
    "\n",
    "### TEST CODE\n",
    "test_filename = \"article.txt\"\n",
    "text = extract_text(test_filename)\n",
    "num_words = len(text.split(' '))\n",
    "num_sent = len(text.split('.'))\n",
    "print(\"Total wordcount:\", num_words)\n",
    "print(\"Total sentences:\", num_sent)\n",
    "requestJson = {\"text\": text}\n",
    "sentimentJson = generate_sentiments(requestJson)\n",
    "# print(sentimentJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentiment_graph(results):    \n",
    "    # visualize the sentiments\n",
    "    sentiment_counts = results.groupby(['Sentiment']).size()\n",
    "    print(sentiment_counts)\n",
    "    fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label=\"\")\n",
    "\n",
    "generate_sentiment_graph(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e415c788c1d92c13de407e6c1b349b076429e99560c057ac37c75124cd2a8b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
