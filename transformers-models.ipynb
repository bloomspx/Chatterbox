{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNA sample article urls\n",
    "url = \"https://www.channelnewsasia.com/sustainability/scientists-discover-how-air-pollution-triggers-lung-cancer-2931111\"\n",
    "# url = \"https://www.channelnewsasia.com/asia/indonesia-subsidised-fuel-price-increase-micro-businesses-2929006\"\n",
    "# url = \"https://cnaluxury.channelnewsasia.com/people/chye-seng-huat-hardware-leon-foo-morning-coffee-machine-206531\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert CNA url -> HTML -> CSV\n",
    "def extract_csv_from_html(url):\n",
    "    try:\n",
    "        page=requests.get(url) \n",
    "    except Exception as e:    \n",
    "        error_type, error_obj, error_info = sys.exc_info()      \n",
    "        print ('ERROR FOR LINK:',url)                     \n",
    "        print (error_type, 'Line:', error_info.tb_lineno)\n",
    "\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "\n",
    "    ## extract article content\n",
    "    textContent = soup.find_all('div', attrs={'class':'text'})\n",
    "    paragraphs = []\n",
    "\n",
    "    for i in textContent:\n",
    "        para = i.find_all('p')\n",
    "        for j in para:\n",
    "            content = j.getText().strip()\n",
    "            paragraphs.append(content)\n",
    "\n",
    "    df = pd.DataFrame(paragraphs)\n",
    "    df.to_csv(\"data/news.csv\", index=False)\n",
    "\n",
    "\n",
    "message = extract_csv_from_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert cna url -> html -> txt\n",
    "def extract_txt_from_html(url):\n",
    "    try:\n",
    "        page=requests.get(url) \n",
    "    except Exception as e:    \n",
    "        error_type, error_obj, error_info = sys.exc_info()      \n",
    "        print ('ERROR FOR LINK:',url)                     \n",
    "        print (error_type, 'Line:', error_info.tb_lineno)\n",
    "\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "    ## extract article title\n",
    "    title = soup.find('h1',attrs={'class':'h1 h1--page-title'}).getText()\n",
    "\n",
    "    ## extract article content\n",
    "    textContent = soup.find_all('div', attrs={'class':'text'})\n",
    "    paragraphs = []\n",
    "\n",
    "    for i in textContent:\n",
    "        para = i.find_all('p')\n",
    "        for j in para:\n",
    "            content = j.getText().strip()\n",
    "            paragraphs.append(content)\n",
    "\n",
    "    single_para = \"\".join(paragraphs)\n",
    "    lines = \"\\n\".join(paragraphs)\n",
    "    multi_para = \"\\n\".join(paragraphs)\n",
    "\n",
    "    f = open(\"article.txt\", \"w+\")\n",
    "    f.write(\"Title: \")\n",
    "    f.write(title)\n",
    "    f.write(\"\\nArticle Text:\\n\\n\")\n",
    "    f.write(multi_para)\n",
    "    f.close()\n",
    "\n",
    "message = extract_txt_from_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform sentinent analysis\n",
    "def get_prediction(texts, model):\n",
    "\n",
    "    new_df = pd.DataFrame(columns=[\"Content\",\"Label\", \"Score\"])\n",
    "\n",
    "    for index in range(len(texts)):\n",
    "        preds = model(texts[index])\n",
    "        print(preds)\n",
    "        pred_sentiment = preds[0][\"label\"]\n",
    "        pred_score = preds[0][\"score\"]\n",
    "\n",
    "        # write data into df\n",
    "        new_df.at[index, \"Label\"] = pred_sentiment\n",
    "        new_df.at[index, \"Score\"] = pred_score\n",
    "        # write text\n",
    "        new_df.at[index, \"Content\"] = \"\".join((texts[index]))\n",
    "\n",
    "    new_df.to_csv(\"data/results.csv\", index=False)\n",
    "    results = new_df\n",
    "    return results\n",
    "\n",
    "def predict(filename):\n",
    "    data = pd.read_csv('data/{}'.format(filename))\n",
    "    texts = data.values.tolist()\n",
    "    message = \"\"\n",
    "    for i in texts:\n",
    "        message += \"\".join(i)\n",
    "    results = get_prediction(texts, nlp)\n",
    "\n",
    "model = \"siebert/sentiment-roberta-large-english\"\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=model)\n",
    "predict(\"news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 70, but you input_length is only 61. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air pollution has long been thought to be linked to a higher risk of lung cancer in people who have never smoked. Scientists say they have identified the mechanism through which air pollution triggers lung cancer. The research has sparked fresh calls for more urgent action to combat climate change.Previous research has shown that the DNA mutations can be present without causing cancer - and that most environmental carcinogens do not cause the mutations. But Swanton's study proposes a different model.When a cell is exposed to pollution it can trigger a \"wound-healing response\" that causes inflammation. And if that cell \"harbours a mutation, it will then form a cancer\"\"This opens a huge door, both for knowledge but also for new ways to prevent\" cancer from developing, says Delaloge. Swanton calls air pollution a \"hidden killer\", pointing to research estimating it is linked to the deaths of more than eight million people a year.\"Given that probably five times as many people are exposed to unhealthy levels of pollution than tobacco, you can see this is quite a major global problem,\" he added.\n"
     ]
    }
   ],
   "source": [
    "# extracts content from csv/txt in /data to an entire paragraph\n",
    "def extract_text(filename):\n",
    "    message = \"\"\n",
    "    if (filename[-4:]) == \".csv\":   # csv format\n",
    "        with open(\"data/{}\".format(filename), 'r', encoding=\"utf8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader, None)  # skip headers\n",
    "            for row in reader:\n",
    "                # print(row)    \n",
    "                message += \"\".join(row)\n",
    "            # print(sentences)\n",
    "    elif (filename[-4:] == \".txt\"):     # txt format\n",
    "        with open(\"data/{}\".format(filename), 'r',  encoding=\"utf8\") as f:\n",
    "            message = \"\".join(f.readlines())\n",
    "    return message\n",
    "\n",
    "# split text > max_length into a list of sentences\n",
    "def form_text_chunks(document, max_length):\n",
    "    chunks = []\n",
    "    sent = \"\"\n",
    "    length = 0\n",
    "    for sentence in document:\n",
    "        # print(sentence + \"\\n\")\n",
    "        sentence +=  \".\"\n",
    "        length += len(sentence)\n",
    "        if length < max_length:\n",
    "            sent += sentence\n",
    "        else:\n",
    "            # print(sent + \"\\n\\n\")\n",
    "            chunks.append(sent)\n",
    "            sent = \"\"\n",
    "            length = 0\n",
    "    if sent:\n",
    "        chunks.append(sent)\n",
    "    return chunks\n",
    "    \n",
    "# summarize text\n",
    "def summarize(summarizer, chunks):\n",
    "    result = \"\"\n",
    "    for i in chunks:\n",
    "        summarized = summarizer(i, max_length=70, min_length=30, do_sample=False)\n",
    "        # print(summarized[0][\"summary_text\"])\n",
    "        result += summarized[0][\"summary_text\"]\n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "def generate_summary(summarizer, filename):\n",
    "    message = extract_text(filename)\n",
    "    sentences = message.split('.')\n",
    "    chunks = form_text_chunks(sentences, 1024)\n",
    "    # print(\"chunks:\", chunks)\n",
    "    result = summarize(summarizer, chunks)\n",
    "    # print(result + \"\\n\")\n",
    "    while (len(result) > 1200):\n",
    "        sentences = message.split('.')\n",
    "        chunks = form_text_chunks(sentences, 1024)\n",
    "        result = summarize(summarizer, chunks)\n",
    "    print(result)\n",
    "    return result\n",
    "    \n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "text = generate_summary(summarizer, \"sample_news.csv\")\n",
    "# text = generate_summary(summarizer, \"summarized.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9985164999961853}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9976512789726257}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9982559084892273}]\n",
      "[{'label': 'POSITIVE', 'score': 0.998354971408844}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9965507984161377}]\n"
     ]
    }
   ],
   "source": [
    "def generate_sentiments(texts, model):\n",
    "    new_df = pd.DataFrame(columns=[\"Content\",\"Label\", \"Score\"])\n",
    "\n",
    "    for index in range(len(texts)):\n",
    "        preds = model(texts[index])\n",
    "        print(preds)\n",
    "        pred_sentiment = preds[0][\"label\"]\n",
    "        pred_score = preds[0][\"score\"]\n",
    "\n",
    "        # write data into df\n",
    "        new_df.at[index, \"Label\"] = pred_sentiment\n",
    "        new_df.at[index, \"Score\"] = pred_score\n",
    "        # write text\n",
    "        new_df.at[index, \"Content\"] = \"\".join((texts[index]))\n",
    "\n",
    "    new_df.to_csv(\"data/results.csv\", index=False)\n",
    "    results = new_df\n",
    "    return results\n",
    "\n",
    "def predict(filename):\n",
    "    message = extract_text(filename)\n",
    "    sentences = message.split('.')\n",
    "    chunks = form_text_chunks(sentences, 1024)\n",
    "    sa_results = generate_sentiments(chunks, nlp)\n",
    "\n",
    "model = \"siebert/sentiment-roberta-large-english\"\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=model)\n",
    "predict(\"sample_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e66abdd767cd4c5625dbaf1187e125ea942cb289be91eec651ecd64fb95f596e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
