{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup, UnicodeDammit\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import pdfplumber\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNA sample article urls\n",
    "# url = \"https://www.channelnewsasia.com/sustainability/scientists-discover-how-air-pollution-triggers-lung-cancer-2931111\"\n",
    "# url = \"https://www.channelnewsasia.com/asia/indonesia-subsidised-fuel-price-increase-micro-businesses-2929006\"\n",
    "url = \"https://www.channelnewsasia.com/singapore/nightclub-operator-club-posh-west-palace-entertainment-evade-gst-money-laundering-jailed-penalty-2935436\"\n",
    "# url = \"https://cnaluxury.channelnewsasia.com/people/chye-seng-huat-hardware-leon-foo-morning-coffee-machine-206531\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert CNA url -> HTML -> CSV\n",
    "def extract_csv_from_html(url):\n",
    "    try:\n",
    "        page=requests.get(url) \n",
    "    except Exception as e:    \n",
    "        error_type, error_obj, error_info = sys.exc_info()      \n",
    "        print ('ERROR FOR LINK:',url)                     \n",
    "        print (error_type, 'Line:', error_info.tb_lineno)\n",
    "\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "\n",
    "    ## extract article content\n",
    "    textContent = soup.find_all('div', attrs={'class':'text'})\n",
    "    paragraphs = []\n",
    "\n",
    "    for i in textContent:\n",
    "        para = i.find_all('p')\n",
    "        for j in para:\n",
    "            content = j.getText().strip()\n",
    "            paragraphs.append(content)\n",
    "\n",
    "    df = pd.DataFrame(paragraphs)\n",
    "    df.to_csv(\"backend/data/news.csv\", index=False)\n",
    "\n",
    "\n",
    "message = extract_csv_from_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert cna url -> html -> txt\n",
    "def extract_txt_from_html(url):\n",
    "    try:\n",
    "        page=requests.get(url) \n",
    "    except Exception as e:    \n",
    "        error_type, error_obj, error_info = sys.exc_info()      \n",
    "        print ('ERROR FOR LINK:',url)                     \n",
    "        print (error_type, 'Line:', error_info.tb_lineno)\n",
    "\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "    ## extract article title\n",
    "    title = soup.find('h1',attrs={'class':'h1 h1--page-title'}).getText()\n",
    "\n",
    "    ## extract article content\n",
    "    textContent = soup.find_all('div', attrs={'class':'text'})\n",
    "    paragraphs = []\n",
    "\n",
    "    for i in textContent:\n",
    "        para = i.find_all('p')\n",
    "        for j in para:\n",
    "            content = j.getText().strip()\n",
    "            paragraphs.append(content)\n",
    "\n",
    "    single_para = \"\".join(paragraphs)\n",
    "    # lines = \"\\n\".join(paragraphs)\n",
    "    # multi_para = \"\\n\".join(paragraphs)\n",
    "\n",
    "    f = open(\"backend/data/article.txt\", \"w+\")\n",
    "    f.write(single_para)\n",
    "    f.close()\n",
    "\n",
    "message = extract_txt_from_html(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts content from csv/txt in /data to an entire paragraph\n",
    "def extract_text(filename):\n",
    "    message = \"\"\n",
    "    name, extension = os.path.splitext(filename)\n",
    "    try: \n",
    "        if extension == \".csv\":   # csv format\n",
    "            with open(\"backend/data/{}\".format(filename), 'r', encoding=\"utf8\") as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader, None)  # skip headers\n",
    "                for row in reader:\n",
    "                    # print(row)    \n",
    "                    message += \"\".join(row)\n",
    "                # print(sentences)\n",
    "        elif extension == \".txt\":     # txt format\n",
    "            with open(\"backend/data/{}\".format(filename), 'rb') as f:\n",
    "                byteString = f.read()\n",
    "                dammit = UnicodeDammit(byteString, [\"utf-8\",  \"ascii\"])\n",
    "                encoding = dammit.original_encoding\n",
    "                print(\"Encoding Type:\", encoding)\n",
    "                message = byteString.decode(encoding, errors='ignore')\n",
    "                # print(message)\n",
    "            \n",
    "        elif extension == '.pdf':\n",
    "            with pdfplumber.open(\"backend/data/{}\".format(filename)) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    message += page.extract_text() \n",
    "    except Exception as err:\n",
    "            print(err, \"occured in\"+filename)\n",
    "    message = message.replace('\\n|\\\\x92\"|\\\\x93|\\\\x94', ' ') # replace common unicode chars\n",
    "    return message\n",
    "\n",
    "# split text > max_length into a list of sentences\n",
    "def form_text_chunks(document, max_length):\n",
    "    chunks = []\n",
    "    sent = \"\"\n",
    "    length = 0\n",
    "    for sentence in document:\n",
    "        # print(sentence + \"\\n\")\n",
    "        sentence +=  \".\"\n",
    "        length += len(sentence)\n",
    "        if length < max_length:\n",
    "            sent += sentence\n",
    "        else:\n",
    "            # print(sent + \"\\n\\n\")\n",
    "            chunks.append(sent)\n",
    "            sent = \"\"\n",
    "            length = 0\n",
    "    if sent:\n",
    "        chunks.append(sent)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize text\n",
    "def summarize(summarizer, chunks):\n",
    "    result = \"\"\n",
    "    for i in chunks:\n",
    "        summarized = summarizer(i, max_length=70, min_length=30, do_sample=False)\n",
    "        # print(summarized[0][\"summary_text\"])\n",
    "        result += summarized[0][\"summary_text\"]\n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "def generate_summary(summarizer, filename):\n",
    "    message = extract_text(filename)\n",
    "    sentences = message.split('.')\n",
    "    chunks = form_text_chunks(sentences, 1024)\n",
    "    # print(\"chunks:\", chunks)\n",
    "    result = summarize(summarizer, chunks)\n",
    "    # print(result + \"\\n\")\n",
    "    while (len(result) > 1200):\n",
    "        sentences = result.split('.')\n",
    "        chunks = form_text_chunks(sentences, 1024)\n",
    "        result = summarize(summarizer, chunks)\n",
    "    print(result)\n",
    "    return result\n",
    "    \n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", tokenizer=\"facebook/bart-large-cnn\")\n",
    "# text = generate_summary(summarizer, \"sample_news.csv\")\n",
    "text = generate_summary(summarizer, \"article.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Type: windows-1250\n",
      "406\n"
     ]
    }
   ],
   "source": [
    "def generate_word_list(filename):\n",
    "    message = extract_text(filename)\n",
    "    num_words = len(message.split(' '))\n",
    "    print(num_words)\n",
    "\n",
    "generate_word_list(\"article.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract NLP data and export as JSON\n",
    "def run_chatterbox(text):\n",
    "    \n",
    "    # Sentiment\n",
    "\n",
    "    # Word Cloud\n",
    "\n",
    "    # Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentiments(message, model):\n",
    "    texts = message.split('.')\n",
    "    # print(texts, \"\\n\\n\")\n",
    "    chunks = form_text_chunks(texts, 512)\n",
    "    new_df = pd.DataFrame(columns=[\"Content\",\"Sentiment\", \"Score\"])\n",
    "\n",
    "    for index in range(len(chunks)):\n",
    "        preds = model(chunks[index])\n",
    "        # print(preds)\n",
    "        pred_sentiment = preds[0][\"label\"]\n",
    "        pred_score = preds[0][\"score\"]\n",
    "\n",
    "        # write predicted data into df\n",
    "        new_df.at[index, \"Sentiment\"] = pred_sentiment\n",
    "        new_df.at[index, \"Score\"] = pred_score\n",
    "        # write text\n",
    "        new_df.at[index, \"Content\"] = \"\".join((chunks[index]))\n",
    "\n",
    "    # new_df.to_csv(\"backend/data/results.csv\", index=False)\n",
    "    new_df.to_json(\"backend/data/results.json\")\n",
    "    results = new_df\n",
    "    return results\n",
    "\n",
    "def predict(filename):\n",
    "    message = extract_text(filename)\n",
    "    # print(message)\n",
    "    results = generate_sentiments(message, nlp)\n",
    "    return results\n",
    "\n",
    "model = \"siebert/sentiment-roberta-large-english\"\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=model)\n",
    "results = predict(\"news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentiment_graph(results):    \n",
    "    # visualize the sentiments\n",
    "    sentiment_counts = results.groupby(['Sentiment']).size()\n",
    "    print(sentiment_counts)\n",
    "    fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label=\"\")\n",
    "\n",
    "generate_sentiment_graph(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(results):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66f0ee19bbaf05b33d576940862416d07a1ba331cace9fd140a372edaed06c71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
